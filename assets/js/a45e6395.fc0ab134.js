"use strict";(globalThis.webpackChunkhumanoid_book=globalThis.webpackChunkhumanoid_book||[]).push([[7645],{6410:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>l,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"chapter4","title":"Chapter 4: Sensing the Physical World","description":"Humanoid robots need to understand their surroundings to interact with them effectively. This chapter explores the diverse range of sensors that enable robots to perceive the physical world, covering everything from vision to touch.","source":"@site/book/chapter4.md","sourceDirName":".","slug":"/chapter4","permalink":"/AI-humanoid-book/book/chapter4","draft":false,"unlisted":false,"editUrl":"https://github.com/Umerqureshi786/AI-humanoid-book/tree/main/book/chapter4.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Chapter 4: Sensing the Physical World","sidebar_position":4},"sidebar":"defaultSidebar","previous":{"title":"Chapter 3: Kinematics, Dynamics, and Control of Bipedal Humanoids","permalink":"/AI-humanoid-book/book/chapter3"},"next":{"title":"Chapter 5: ROS 2 as the Robotic Nervous System","permalink":"/AI-humanoid-book/book/chapter5"}}');var i=o(4848),r=o(8453);const s={title:"Chapter 4: Sensing the Physical World",sidebar_position:4},a="Chapter 4: Sensing the Physical World",c={},d=[{value:"Vision, Depth, and Tactile Sensing",id:"vision-depth-and-tactile-sensing",level:2},{value:"Proprioception and IMUs",id:"proprioception-and-imus",level:2},{value:"Sensor Fusion",id:"sensor-fusion",level:2}];function h(e){const n={h1:"h1",h2:"h2",header:"header",p:"p",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-4-sensing-the-physical-world",children:"Chapter 4: Sensing the Physical World"})}),"\n",(0,i.jsx)(n.p,{children:"Humanoid robots need to understand their surroundings to interact with them effectively. This chapter explores the diverse range of sensors that enable robots to perceive the physical world, covering everything from vision to touch."}),"\n",(0,i.jsx)(n.h2,{id:"vision-depth-and-tactile-sensing",children:"Vision, Depth, and Tactile Sensing"}),"\n",(0,i.jsx)(n.p,{children:"Vision is arguably the most powerful sense for robots, providing rich information about the environment. We will cover RGB cameras, depth sensors (like LiDAR and structured light), and how they are used to build 3D models of the world. Tactile sensing, providing information about contact and force, is also crucial for dexterous manipulation and safe interaction."}),"\n",(0,i.jsx)(n.h2,{id:"proprioception-and-imus",children:"Proprioception and IMUs"}),"\n",(0,i.jsx)(n.p,{children:"Proprioception refers to a robot's sense of its own body position and movement. This internal sensing is critical for control and balance. We will discuss joint encoders, force-torque sensors, and Inertial Measurement Units (IMUs) that provide data on orientation and acceleration."}),"\n",(0,i.jsx)(n.h2,{id:"sensor-fusion",children:"Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Each sensor provides a unique, often complementary, view of the world. Sensor fusion is the process of combining data from multiple sensors to obtain a more complete and accurate understanding than any single sensor could provide. This section will cover techniques for fusing data from heterogeneous sensors, addressing issues like calibration, synchronization, and uncertainty."})]})}function l(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>a});var t=o(6540);const i={},r=t.createContext(i);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);