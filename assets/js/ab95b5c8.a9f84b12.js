"use strict";(globalThis.webpackChunkhumanoid_book=globalThis.webpackChunkhumanoid_book||[]).push([[8970],{6082:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter7","title":"Chapter 7: Perception for Humanoids","description":"Humanoid robots, like humans, rely heavily on perception to navigate, interact, and understand their surroundings. This chapter focuses on advanced perception techniques tailored for humanoid platforms, addressing the unique challenges and opportunities presented by their form factor and operational environments.","source":"@site/book/chapter7.md","sourceDirName":".","slug":"/chapter7","permalink":"/AI-humanoid-book/book/chapter7","draft":false,"unlisted":false,"editUrl":"https://github.com/Umerqureshi786/AI-humanoid-book/tree/main/book/chapter7.md","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Chapter 7: Perception for Humanoids","sidebar_position":7},"sidebar":"defaultSidebar","previous":{"title":"Chapter 6: High-Fidelity Simulation","permalink":"/AI-humanoid-book/book/chapter6"},"next":{"title":"Chapter 8: Locomotion and Balance","permalink":"/AI-humanoid-book/book/chapter8"}}');var o=i(4848),r=i(8453);const a={title:"Chapter 7: Perception for Humanoids",sidebar_position:7},s="Chapter 7: Perception for Humanoids",l={},c=[{value:"Visual SLAM",id:"visual-slam",level:2},{value:"Scene Understanding",id:"scene-understanding",level:2},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2}];function d(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-7-perception-for-humanoids",children:"Chapter 7: Perception for Humanoids"})}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots, like humans, rely heavily on perception to navigate, interact, and understand their surroundings. This chapter focuses on advanced perception techniques tailored for humanoid platforms, addressing the unique challenges and opportunities presented by their form factor and operational environments."}),"\n",(0,o.jsx)(n.h2,{id:"visual-slam",children:"Visual SLAM"}),"\n",(0,o.jsx)(n.p,{children:"Simultaneous Localization and Mapping (SLAM) is a cornerstone of robot navigation, allowing a robot to build a map of an unknown environment while simultaneously localizing itself within that map. For humanoids, visual SLAM (using cameras) is particularly relevant. We will cover:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature-based vs. Direct SLAM"}),": Different approaches to extracting information from visual data."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Monocular, Stereo, and RGB-D SLAM"}),": Techniques leveraging different camera types."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Loop Closure Detection"}),": Recognizing previously visited locations to correct for accumulated errors."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"scene-understanding",children:"Scene Understanding"}),"\n",(0,o.jsx)(n.p,{children:'Beyond simply knowing "where am I?", humanoids need to understand "what is around me?" and "what can I do with it?". Scene understanding involves interpreting perceived data to derive semantic information about objects, their properties, and their affordances (what actions they enable). This section will discuss:'}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Object Detection and Recognition"}),": Identifying known objects in the environment."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Semantic Segmentation"}),": Classifying every pixel in an image according to the object it belongs to."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Affordance Learning"}),": Inferring possible interactions with objects."]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,o.jsx)(n.p,{children:"Training robust perception models, especially deep learning models, requires vast amounts of labeled data. Collecting and annotating real-world data for complex humanoid tasks can be prohibitive. Synthetic data generation, leveraging high-fidelity simulators like Isaac Sim, offers a scalable solution. We will explore:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Domain Randomization"}),": Randomizing simulation parameters to improve transferability to the real world."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Procedural Content Generation"}),": Automatically creating diverse scenes and objects."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ground Truth Extraction"}),": Easily obtaining perfect labels (e.g., object poses, semantic masks) directly from the simulator."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var t=i(6540);const o={},r=t.createContext(o);function a(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);